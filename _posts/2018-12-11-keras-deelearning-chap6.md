---
layout: post
title: "텍스트와 시퀀스를 위한 딥러닝"
date: 2018-12-11
excerpt: "텍스트와 시퀀스를 위한 딥러닝"
tags: [Keras, deepLearning]
comments: true
---

# 6. 텍스트와 시퀀스를 위한 딥러닝

## 6.1 텍스트 데이터 다루기

* 텍스트는 가장 흔한 시퀀스 형태의 데이터
* 자연어 처리를 위한 딥러닝은 단어, 문장, 문단에 적용한 패턴 인식
* 텍스트 벡터화: 텍스트를 수치형 텐서로 변환하는 과정
	1. 텍스트를 단어로 나누고 각 단어를 하나의 벡터로 변환
	2. 텍스트를 문자로 나누고 각 문자를 하나의 벡터로 변환
	3. 텍스트에서 단어나 문자의 n-gram을 추출하여 각 n-그램을 하나의 벡터로 변환합니다.
* 토큰: 텍스트를 나누는 단위
* 토큰화: 텍스트를 토큰으로 나누는 작업

* 6.1.1 단어와 문자의 원-핫 인코딩
	* 토큰을 벡터로 변환하는 가장 일반적이고 기본적인 방법
	* 케라스에는 윈본 텍스트 데이터를 단어 또는 문자 수준의 원-핫 인코딩으로 변환해주는 유틸리티 존재
	* 원-핫 해싱 기법: 각 단어에 명시적으로 인덱스를 할당하고 이 인덱스를 딕셔너리에 저장하는 대신에 단어를 해싱하여 고정된 크기의 벡터로 변환

* 6.1.2 단어 임베딩 사용하기
	* 저차원의 실수형 벡터
	* Embedding 층을 사용하여 단어 임베딩 학습하기
		* 단어와 밀집 벡터를 연관 짓는 가장 간단한 방법은 랜덤하게 벡터를 선택하는 것
		* 단어 벡터 사이에 좀 더 추상적이고 기하학적인 관계를 얻으려면 단어 사이에 있는 의미 관계를 반영해야 함
		* 일반적으로 두 단어 벡터 사이의 거리는 이 단어 사이의 의미 거리와 관계되어 있다.
		* 새로운 작업에는 새로운 임베딩을 학습하는 것이 타당
		* Embedding 층은 정수 인덱스를 밀집 벡터로 매핑하는 딕셔너리
		* 배치에 있는 모든 시퀀스는 길이가 같아야 하므로 작은 길이의 시퀀스는 0으로 패딩되고 길이가 더 긴 시퀀스는 잘림
		* Embedding층은 크기가 (samples, sequence_length, embedding_dimensinality)인 3D 실수형 텐서 반환
		* Embedding층의 가중치는 랜덤하게 초기화
	* 사전 훈련된 단어 임베딩 사용하기
		* 풀려는 문제와 함께 단어 임베딩을 학습하는 대신에 미리 계산된 임베딩 공간에서 임베딩 벡터를 로드할 수 있음
* 6.1.3 모든 내용을 적용하기: 원본 텍스트에서 단어 임베딩까지
* 6.1.4 정리
	* 원본 텍스트를 신경망이 처리할 수 있는 형태로 변환
	* 케라스 모델에 Embedding 층을 추가하여 어떤 작업에 특화된 토큰 임베딩을 학습
	* 데이터가 부족한 자연어 처리 문제에서 사전 훈련된 단어 임베딩을 사용하여 성능 향상을 꾀함

## 6.2 순환 신경망 이해하기

* 피드포워드 네트워크(feedforward network): 전체 시퀀스를 하나의 데이터 포인트로 변환(완전 연결 신경망 + 합성곱 신경망)
* 순환 신경망(Recurrent Neural Network): 시퀀스의 원소를 순회하면서 지금까지 처리한 정보를 상태에 저장
* RNN은 반복할 때 이전에 계산한 정보를 재사용하는 for 루프
* 6.2.1 케라스의 순환 층
	* simpleRNN 사용
	* 전체 시퀀스가 아니라 처음 500개의 단어만 입력에 사용했기 때문에 성능이 좋지 않음
	* simpleRNN이 텍스트처럼 긴 시퀀스를 처리하는 데 적합하지 않음
* 6.2.2 LSTM과 GRU 층 이해하기
	* simpleRNN은 신 시간에 걸친 의존성은 학습할 수 없는 문제가 있다.(그래디언트 소실 문제)
	* LSTM(Long Short-Term Memory)
		* SimpleRNN에서 정보를 여러 타임스텝에 결쳐 나르는 방법이 추가
		* 시퀀스 어느 지점에서 추출된 정보가 컨베이어 벨트 위로 올라가 필요한 시점의 타임스텝으로 이동하여 떨굼
		* 나중을 위해 정보를 저장함으로써 처리 과정에서 오래된 시그널이 점차 소실되는 것을 막아 줌
		* LSTM 셀의 구체적인 구조에 대해 이해할 필요가 전혀 없음
		* 과거를 나중에 다시 주입하여 그래디언트 소실 문제를 해결한다는 역할만 기억하면 됨
* 6.2.3 케라스를 사용한 LSTM 예제
	* 많은 계산을 사용한 것치고 획기적인 결과는 아님
		* 하이퍼파라미터를 전혀 튜닝하지 않았기 때문
		* 규제가 없기 때문
		* 리뷰를 전체적으로 길게 분석하는 것은 감성 분류 문제에 도움이 되지 않기 때문에
	* LSTM은 감성 분류 문제보다는 훨씬 더 복잡한 자연어 처리문제들(질문 응답, 기계 번역)에서 능력이 드러남
* 6.2.4 정리
	* RNN이 무엇이고 동작하는 방법
	* LSTM이 무엇이고 긴 시퀀스에서 단순한 RNN보다 더 잘 작동하는 이유
	* 케라스의 RNN 층을 사용하여 시퀀스 데이터를 처리하는 방법

## 순환 신경망의 고급 사용법

* 온도, 기압, 습도를 이용하여 마지막 데이터 포인트에서 24시간 이후의 온도를 예측
* 순환 드롭아웃(recurrent dropout): 순환 층에서 과대적합을 방지하기 위해 케라스에 내장되어 있는 드롭아웃 사용
* 스태킹 순환 충(stacking recurrent layer): 네트워크의 표현 능력(representational layer)를 증가시킨다.(대신 계산 비용이 많이 든다.)
* 양방향 순환 층(bidirectional recurrent layer): 순환 네트워크에 같은 정보를 다른 방향으로 주입하여 정확도를 높이고 기억을 좀 더 오래 유지시킨다.

* 6.3.1 기온 예측 문제
